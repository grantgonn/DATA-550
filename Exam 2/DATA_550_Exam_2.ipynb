{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (20 points) \n",
    "\n",
    "Please state **True** or **False** for the below statements.\n",
    "\n",
    "- (a) Version control of both code and data is crucial before moving a model to production. \n",
    "\n",
    "- (b) A/B testing is a commonly used technique to deploy multiple models and compare their performance in production.\n",
    "\n",
    "- (c) Data drift refers to changes in the input data distribution over time.\n",
    "\n",
    "- (d) From the model governance perspective, auditability and explainability are optional in regulated industries. \n",
    "\n",
    "- (e) Model performance on training data is a sufficient metric for production readiness. \n",
    "\n",
    "- (f) Continuous Integration/Continuous Deployment (CI/CD) is not applicable to machine learning workflows.\n",
    "\n",
    "- (g) Feedback loops in MLOps allow models to retrain based on newly collected data.\n",
    "\n",
    "- (h) Model governance also includes managing access controls for models and data. \n",
    "\n",
    "- (i) Model performance in production never degrades over time if trained properly. \n",
    "\n",
    "- (j) Reproducibility is a key requirement when preparing ML models for production. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. True\n",
    "\n",
    "b. True\n",
    "\n",
    "c. True\n",
    "\n",
    "d. False\n",
    "\n",
    "e. False\n",
    "\n",
    "f. False\n",
    "\n",
    "g. True\n",
    "\n",
    "h. True\n",
    "\n",
    "i. False\n",
    "\n",
    "j. True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (4 points)\n",
    "\n",
    "Which of the following is a primary goal of monitoring a machine learning model in production?\n",
    "\n",
    "- (a) Reduce training time\n",
    "- (b) Prevent overfitting during training\n",
    "- (c) Detect model drift and performance issues \n",
    "- (d) Speed up batch processing\n",
    "- (e) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (5 points)\n",
    "\n",
    "Explain the difference between data drift and concept drift. Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data drift is the change in the input data distributions over time while concept drift is the change in the relationship between input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (4 points)\n",
    "\n",
    "Which tool is typically used to track experiments and ensure governance?\n",
    "\n",
    "- (a) Scikit-learn\n",
    "- (b) TensorFlow\n",
    "- (c) MLflow \n",
    "- (d) NumPy\n",
    "- (e) All fo the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (5 points)\n",
    "\n",
    "What is the purpose of model governance in MLOps? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model governance ensures machine learning models are developed, deployed, and maintained so that they meet business, regulatory, and ethical standards for the lifetime of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (4 points)\n",
    "\n",
    "Which of the following is a best practice before moving a model to production?\n",
    "\n",
    "- (a) Skip testing to save time\n",
    "- (b) Ignore reproducibility\n",
    "- (c) Use version control for code and data \n",
    "- (d) Train with only a small dataset\n",
    "- (e) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (5 points)\n",
    "\n",
    "Why is it important to monitor machine learning models in production? Please, be specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to detect data and concept drift early, ensure model performace is up to business standards, and identify and bias that may present itself overtime. Consistant monitoring of ML models can help resolve these issues before they become a major problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (4 points)\n",
    "\n",
    "Why is reproducibility important in MLOps?\n",
    "\n",
    "- (a) It improves model accuracy\n",
    "- (b) It ensures consistent results across environments \n",
    "- (c) It reduces data storage needs\n",
    "- (d) It accelerates training time\n",
    "- (e) None of the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 (5 points)\n",
    "\n",
    "How does governance support responsible AI practices? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It insures transparency, reduces bias, and ensures compliance with regulatory and ethics standards in the AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 (4 points)\n",
    "\n",
    "What is data drift?\n",
    "\n",
    "- (a) When model predictions slowly become more accurate\n",
    "- (b) When input data changes over time, affecting model performance \n",
    "- (c) When a model begins to overfit\n",
    "- (d) When training data has missing values\n",
    "- (e) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 (5 points)\n",
    "\n",
    "What information should be captured for model versioning? Please, be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model verison, hyperparameters, training data information such as any preprocessing done, and performance metrics are all important information to be captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12 \n",
    "\n",
    "Consider the `SeoulBikeData.csv`. This data comes from a bike rental app in Seoul, South Korea. This data contains weather information, such as temperature, humity, and others. This bike rental app is interested on predicting `Rented Bike Count`. The goal is to build regression models to predict `Rented Bike Count`.  \n",
    "\n",
    "### Exercise 12(a) (10 points)\n",
    "\n",
    "Create a `.py` that cleans and prepare the data for modeling purpose. The script should perform the following:\n",
    "\n",
    "- Read the `SeoulBikeData.csv` file. If you run to issues reading the data, consider adding `encoding=\"latin1\"` as one of the arguments in `read_csv`.\n",
    "- 0-1 encode the categorical features.\n",
    "- Explore the data\n",
    "   - Create a a few exploratory charts.\n",
    "   - Report any anomalies in data if there are any.\n",
    "- Engineer at least three feature from `Date`.\n",
    "- Split the data into `train` (80%) and `test` (20%). Save both data-frames as `train.csv` and `test.csv`, respectively.\n",
    "\n",
    "### Exercise 12(b) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`) and log the best `RandomForestRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. Log the permutation importance in `mlflow` and save the hyper-parameters in a `json` file.\n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `RandomForestRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 12(c) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`) and log the best `ExtraTreesRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. Log the permutation importance in `mlflow` and save the hyper-parameters in a `json` file.\n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `ExtraTreesRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 12(d) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`) and log the best `GradientBoostingRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. Log the permutation importance in `mlflow` and save the hyper-parameters in a `json` file.\n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `GradientBoostingRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 12(e) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`, tune `n_estimators`, `max_depth`, `learning_rate`, `colsample_bytree`, `gamma`, `reg_alpha`, `reg_lambda`, and `min_child_weight`) and log the best `XGBRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. Log the permutation importance in `mlflow` and save the hyper-parameters in a `json` file.\n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `XGBRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 12(f) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`, tune `n_estimators`, `max_depth`, `learning_rate`, `num_leaves`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, and `min_child_samples`) and log the best `LGBMRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. Log the permutation importance in `mlflow` and save the hyper-parameters in a `json` file.\n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `LGBMRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 12(g) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`, tune `iterations`, and `max_depth`) and log the best `CatBoostRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. Log the permutation importance in `mlflow` and save the hyper-parameters in a `json` file.\n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `CatBoostRegressor`, then log the model and its performance. \n",
    "\n",
    "### Exercise 12(h) (15 points) \n",
    "Create a `.py` file that trains a `VotingRegressor` model using the best models from sections 12(b) to 12(g). Load the hyperparameters from the `json` files and set these models as the base models in the `VotingRegressor`. Find the optimal weights in the `VotingRegressor` using `Optuna`. Log the out-of-sample performance using `root_mean_squared_error` over 5-fold cross-validation in `mlflow`.\n",
    "\n",
    "### Exercise 12(i) (15 points)\n",
    "\n",
    "Create a `.py` file that trains a `StackingRegressor` model using the best models from parts 12(b) to 12(g) as inputs. Load the hyperparameters from the `json` files and define the selected models as the base models in the `StackingRegressor`, while using `Ridge` as the meta-learner. Log the out-of-sample performance, measured by `root_mean_squared_error`, across 5 folds in `mlflow`.\n",
    "\n",
    "### Exercise 12(j) (15 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` and `test.csv` data files. Using `evidently`, generate a data drift report comparing the `train` and `test` datasets. Log the results of the data drift analysis in `mlflow`.\n",
    "\n",
    "### Exercise 12(k) (3 points)\n",
    "\n",
    "Create a shell file that runs the scripts from parts 12(b) to 12(j).\n",
    "\n",
    "### Exercise 12(l) (7 points)\n",
    "\n",
    "Create a `.py` file that reads the `test.csv` data file. Load the best models from sections 12(b) to 12(i) to make predictions on the `test` dataset, and report their corresponding root mean squared error (RMSE). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
