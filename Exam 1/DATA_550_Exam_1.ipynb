{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (20 points) \n",
    "\n",
    "Please state **True** or **False** for the below statements.\n",
    "\n",
    "- (a) MLOps primarily focuses on deploying machine learning models and does not involve model monitoring or retraining. \n",
    "\n",
    "- (b) In MLOps, model drift refers to changes in the underlying data distribution that can degrade model performance over time. \n",
    "\n",
    "- (c) MLOps encourages the use of version control for both data and models to track changes and ensure reproducibility.\n",
    "\n",
    "- (d) In MLOps, developing a model is a one-time process, and once deployed, the model does not require retraining. \n",
    "\n",
    "- (e) The main goal of MLOps is to replace data scientists with automated pipelines. \n",
    "\n",
    "- (f) MLOps encourages the use of automated pipelines for model training, evaluation, and deployment to improve efficiency and reproducibility.\n",
    "\n",
    "- (g) Model versioning in MLOps allows teams to track different iterations of a model and roll back to a previous version if needed. \n",
    "\n",
    "- (h) Data validation and quality checks are not necessary in MLOps because model performance is only dependent on the algorithm used. \n",
    "\n",
    "- (i) Model explainability is a critical aspect of MLOps, especially for regulated industries like finance and healthcare. \n",
    "\n",
    "- (j) Continuous Integration (CI) and Continuous Deployment (CD) are key components of MLOps to automate model training and deployment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. False\n",
    "\n",
    "b. True\n",
    "\n",
    "c. True\n",
    "\n",
    "d. False\n",
    "\n",
    "e. False\n",
    "\n",
    "f. True\n",
    "\n",
    "g. True\n",
    "\n",
    "h. False\n",
    "\n",
    "i. True\n",
    "\n",
    "j. True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (4 points)\n",
    "\n",
    "Which of the following is NOT a key benefit of using MLOps?\n",
    "\n",
    "- (a) Improved collaboration between data scientists and engineers\n",
    "- (b) Faster and more reliable model deployment\n",
    "- (c) Guaranteed model accuracy over time \n",
    "- (d) Automated model monitoring and retraining\n",
    "- (e) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (5 points)\n",
    "\n",
    "Explain the role of model monitoring in MLOps and why it is essential after deployment. Be specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitoring is important to ensure the model's performance remains up to standard throughout its lifetime. After deployment data drift can be an issue and monitoring the models is important so data issues can be caught and resolved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (4 points)\n",
    "\n",
    "Which component in an MLOps pipeline is responsible for detecting changes in input data distribution?\n",
    "\n",
    "- (a) Drift detection system \n",
    "- (b) Feature store\n",
    "- (c) Model registry\n",
    "- (d) Feature selection\n",
    "- (e) All of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (5 points)\n",
    "\n",
    "What are the key differences between traditional machine learning workflows and MLOps practices?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaboration, automation, version control, monitoring, and scalability are all aimed to be optimised in MLOps "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (4 points)\n",
    "\n",
    "What is the main purpose of a Feature Store in an MLOps workflow?\n",
    "\n",
    "- (a) To store precomputed features and ensure consistency across training and inference \n",
    "- (b) To store trained model artifacts for deployment\n",
    "- (c) To manage hyperparameter tuning experiments\n",
    "- (d) To automate real-time model inference\n",
    "- (e) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (5 points)\n",
    "\n",
    "Describe how CI/CD pipelines work in MLOps and how they differ from traditional software CI/CD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI/CD pipelines are designed to automate and streamline the process of building, testing, deploying, and monitoring ML models in a production enviornment. Traditional CI/CD Primarily focuses on ensuring that the software works correctly and has less emphasis on managing ML models and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (4 points)\n",
    "\n",
    "Which of the following is **NOT** a common technique used in feature engineering for MLOps?\n",
    "\n",
    "- (a) One-hot encoding\n",
    "- (b) Feature scaling\n",
    "- (c) Hyperparameter tunig \n",
    "- (d) Imputation of missing values\n",
    "- (e) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 (5 points)\n",
    "\n",
    "Explain why feature selection is important in MLOps and describe two different techniques used for selecting the most relevant features. Provide an example for each technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is important in MLOps because it helps identify the most relevant and useful features for model training. In some cases, unnecessary features can add unneeded noise to the model, and removing them can improve the performance and efficiency of the model. Doing a chi-squared test can identify important features based on a statistical test, also RFE recursively evaluates subsets of features by training and testing the model and selecting the best-performing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10 (4 points)\n",
    "\n",
    "What is the primary goal of feature selection in MLOps?\n",
    "\n",
    "- (a) To increase the number of features to improve model complexity\n",
    "- (b) To remove redundant or irrelevant features to enhance model performance \n",
    "- (c) To add synthetic features for better generalization\n",
    "- (d) To modify feature values to fit a normal distribution\n",
    "- (e) None of the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11 \n",
    "\n",
    "Consider the `SeoulBikeData.csv`. This data comes from a bike rental app in Seoul, South Korea. This data contains weather information, such as temperature, humity, and others. This bike rental app is interested on predicting `Rented Bike Count`. The goal is to build regression models to predict `Rented Bike Count`.  \n",
    "\n",
    "### Exercise 11(a) (10 points)\n",
    "\n",
    "Create a `.py` that cleans and prepare the data for modeling purpose. The script should perform the following:\n",
    "\n",
    "- Read the `SeoulBikeData.csv` file. If you run to issues reading the data, consider adding `encoding=\"latin1\"` as one of the arguments in `read_csv`.\n",
    "- 0-1 encode the categorical features.\n",
    "- Explore the data\n",
    "   - Create a a few exploratory charts.\n",
    "   - Report any anomalies in data if there are any.\n",
    "- Engineer at least three feature from `Date`.\n",
    "- Split the data into `train` (80%) and `test` (20%). Save both data-frames as `train.csv` and `test.csv`, respectively.\n",
    "\n",
    "### Exercise 11(b) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`) and log the best `RandomForestRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. \n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `RandomForestRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 11(c) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`) and log the best `ExtraTreesRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. \n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `ExtraTreesRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 11(d) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`) and log the best `GradientBoostingRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. \n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `GradientBoostingRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 11(e) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`, tune `n_estimators`, `max_depth`, `learning_rate`, `colsample_bytree`, `gamma`, `reg_alpha`, `reg_lambda`, and `min_child_weight`) and log the best `XGBRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. \n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `XGBRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 11(f) (20 points)\n",
    "\n",
    "Create a `.py` file that reads the `train.csv` data file, tunes (using `Optuna`, tune `n_estimators`, `max_depth`, `learning_rate`, `num_leaves`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, and `min_child_samples`) and log the best `LGBMRegressor` model out-of-sample performance (using `root_mean_squared_error`) over 5-folds in `mlflow`. Then, run the `permutation_importance` algorithm. \n",
    "\n",
    "- If you are able to build a model with less number of features (based on the permutation importance scores) that outperforms the best `LGBMRegressor`, then log the model and its performance.\n",
    "\n",
    "### Exercise 11(g) (3 points)\n",
    "\n",
    "Create a shell file that runs the scripts from parts 12(b) to 12(f).\n",
    "\n",
    "### Exercise 11(h) (7 points)\n",
    "\n",
    "Create a `.py` file that reads the `test.csv` data file. Load the best models from sections 12(b) to 12(f) to make predictions on the `test` dataset, and report their corresponding root mean squared error (RMSE). Additionally, report the RMSE of the average ensemble, and weighted average of the models (use the models' RMSEs to compute the weights) from sections 12(b) to 12(f)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
